import pandas as pd
import jieba
import re
import os

# -------------------------- 1. è‡ªåŠ¨å®šä½Excelæ–‡ä»¶ï¼ˆæ´›å¤©ä¾/èµµä¸½é¢–ï¼‰ --------------------------
def find_idol_excel():
    """æ‰«æå½“å‰ç›®å½•ï¼Œæ‰¾åˆ°å«æ´›å¤©ä¾/èµµä¸½é¢–çš„Excelæ–‡ä»¶"""
    excel_map = {}
    current_dir = os.getcwd()
    for file in os.listdir(current_dir):
        if file.endswith(".xlsx"):
            if "æ´›å¤©ä¾" in file:
                excel_map["æ´›å¤©ä¾"] = os.path.join(current_dir, file)
            elif "èµµä¸½é¢–" in file:
                excel_map["èµµä¸½é¢–"] = os.path.join(current_dir, file)
    if not excel_map:
        raise FileNotFoundError("æœªæ‰¾åˆ°æ´›å¤©ä¾/èµµä¸½é¢–çš„Excelæ–‡ä»¶ï¼è¯·ç¡®è®¤æ–‡ä»¶åå«å¶åƒå")
    print("âœ… æ‰¾åˆ°ç›®æ ‡Excelï¼š")
    for idol, path in excel_map.items():
        print(f"- {idol}ï¼š{os.path.basename(path)}")
    return excel_map

# -------------------------- 2. æ–‡æœ¬æ¸…æ´—+åˆ†è¯æ ¸å¿ƒå‡½æ•° --------------------------
def clean_comment(comment):
    """æ¸…æ´—å•æ¡ä¸­æ–‡è¯„è®ºï¼šå»ç©ºå€¼ã€ç‰¹æ®Šå­—ç¬¦ã€å¤šä½™ç©ºæ ¼"""
    if pd.isna(comment) or str(comment).strip() == "":
        return None
    # ä»…ä¿ç•™ä¸­æ–‡ï¼Œå»é™¤æ‰€æœ‰ç¬¦å·/æ•°å­—/è‹±æ–‡
    cleaned = re.sub(r"[^\u4e00-\u9fa5\s]", "", str(comment))
    cleaned = re.sub(r"\s+", " ", cleaned).strip()
    return cleaned if cleaned else None

def get_raw_tokens(cleaned_comment):
    """ä»…åˆ†è¯ï¼ˆä¿ç•™åœç”¨è¯ï¼‰â†’ è¿”å›å•è¯åˆ—è¡¨"""
    if not cleaned_comment:
        return []
    return jieba.lcut(cleaned_comment, cut_all=False)  # ç²¾ç¡®åˆ†è¯

def get_filtered_tokens(cleaned_comment):
    """åˆ†è¯+å»åœç”¨è¯ â†’ è¿”å›å•è¯åˆ—è¡¨"""
    if not cleaned_comment:
        return []
    # 1. å…ˆåˆ†è¯
    tokens = jieba.lcut(cleaned_comment, cut_all=False)
    # 2. ä¸­æ–‡åœç”¨è¯åº“ï¼ˆæ ¸å¿ƒæ— æ„ä¹‰è¯ï¼‰
    stop_words = set([
        "çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", "æˆ‘ä»¬", "ä½ ä»¬", "ä»–ä»¬",
        "è¿™", "é‚£", "æ­¤", "å½¼", "å’Œ", "ä¸", "åŠ", "æˆ–", "ä½†", "è€Œ", "å´", "è‹¥",
        "å› ä¸º", "æ‰€ä»¥", "è™½ç„¶", "ä½†æ˜¯", "å¦‚æœ", "åªè¦", "åªæœ‰", "ç”±äº", "å› æ­¤",
        "åœ¨", "äº", "åˆ°", "ä»", "å‘", "å¯¹", "å¯¹äº", "å…³äº", "æŠŠ", "è¢«", "è®©",
        "èƒ½", "ä¼š", "å¯ä»¥", "å¯èƒ½", "åº”è¯¥", "å¿…é¡»", "éœ€è¦", "è¦", "ä¸è¦", "æ²¡",
        "ä¸", "æ²¡", "æ— ", "é", "å¦", "åˆ«", "å¾ˆ", "éå¸¸", "å¤ª", "æ›´", "æœ€", "æ¯”è¾ƒ",
        "è¿˜", "ä¹Ÿ", "åˆ", "å†", "æ‰", "å°±", "éƒ½", "å…¨", "æ€»", "å…±", "æ‰€æœ‰",
        "ä¸€ä¸ª", "ä¸€äº›", "ä¸€ç‚¹", "ä¸€æ ·", "ä¸€èµ·", "ä¸€ç›´", "ä¸€å®š", "ä¸€èˆ¬",
        "å•Š", "å‘€", "å‘¢", "å—", "å§", "å•¦", "å“¦", "å“‡", "å”‰"
    ])
    # 3. è¿‡æ»¤åœç”¨è¯+å•å­—
    return [t for t in tokens if t not in stop_words and len(t)>=2]

# -------------------------- 3. ç”Ÿæˆç¤ºä¾‹æ ¼å¼çš„TXTï¼ˆæ¯è¡Œä¸€ä¸ªè¯ï¼‰ --------------------------
def generate_txt_by_idol(idol_name, excel_path):
    """ä¸ºå•ä¸ªå¶åƒç”Ÿæˆä¸¤ä¸ªTXTï¼šä»…åˆ†è¯.txt + å»åœç”¨è¯.txt"""
    print(f"\n===== å¤„ç†ã€{idol_name}ã€‘=====")
    # è¯»å–Excel
    df = pd.read_excel(excel_path, engine="openpyxl")
    print(f"1. åŸå§‹æ•°æ®ï¼š{len(df)}æ¡")
    
    # é€‰æ‹©è¯„è®ºåˆ—
    print(f"\nè¯„è®ºåˆ—é€‰æ‹©ï¼ˆè¾“å…¥åºå·ï¼‰ï¼š")
    for i, col in enumerate(df.columns, 1):
        print(f"   {i}. {col}")
    col_idx = int(input("   è¾“å…¥è¯„è®ºåˆ—åºå·ï¼š")) - 1
    comment_col = df.columns[col_idx]
    
    # æ¸…æ´—è¯„è®º
    df["æ¸…æ´—å"] = df[comment_col].apply(clean_comment)
    df_valid = df.dropna(subset=["æ¸…æ´—å"]).reset_index(drop=True)
    print(f"2. æœ‰æ•ˆè¯„è®ºï¼š{len(df_valid)}æ¡")
    
    # æ”¶é›†æ‰€æœ‰åˆ†è¯ç»“æœï¼ˆæ‰å¹³åŒ–ï¼šæ‰€æœ‰è¯„è®ºçš„è¯åˆå¹¶æˆä¸€ä¸ªåˆ—è¡¨ï¼‰
    all_raw_tokens = []    # å«åœç”¨è¯çš„æ‰€æœ‰è¯
    all_filtered_tokens = []  # å»åœç”¨è¯çš„æ‰€æœ‰è¯
    for comment in df_valid["æ¸…æ´—å"]:
        all_raw_tokens.extend(get_raw_tokens(comment))
        all_filtered_tokens.extend(get_filtered_tokens(comment))
    
    # ç”ŸæˆTXTï¼ˆæ¯è¡Œä¸€ä¸ªè¯ï¼Œå’Œå‚è€ƒæ ¼å¼å®Œå…¨ä¸€è‡´ï¼‰
    raw_txt = f"{idol_name}_ä»…åˆ†è¯ç»“æœ.txt"
    filtered_txt = f"{idol_name}_å»é™¤åœç”¨è¯ä¹‹åç»“æœ.txt"
    
    # ä¿å­˜ä»…åˆ†è¯ï¼ˆå«åœç”¨è¯ï¼‰
    with open(raw_txt, "w", encoding="utf-8") as f:
        for token in all_raw_tokens:
            f.write(token + "\n")
    # ä¿å­˜å»åœç”¨è¯å
    with open(filtered_txt, "w", encoding="utf-8") as f:
        for token in all_filtered_tokens:
            f.write(token + "\n")
    
    print(f"3. æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼š")
    print(f"   - {raw_txt}ï¼ˆå…±{len(all_raw_tokens)}ä¸ªè¯ï¼Œå«åœç”¨è¯ï¼‰")
    print(f"   - {filtered_txt}ï¼ˆå…±{len(all_filtered_tokens)}ä¸ªè¯ï¼Œæ— åœç”¨è¯ï¼‰")

# -------------------------- 4. ä¸»æµç¨‹ --------------------------
if __name__ == "__main__":
    try:
        excel_map = find_idol_excel()
        for idol, path in excel_map.items():
            generate_txt_by_idol(idol, path)
        print("\nğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼æ¯ä¸ªå¶åƒç”Ÿæˆ2ä¸ªTXTï¼ˆæ¯è¡Œä¸€ä¸ªè¯ï¼‰ï¼š")
        print("   æ ¼å¼å®Œå…¨åŒ¹é…å‚è€ƒç¤ºä¾‹ï¼Œå¯ç›´æ¥æ‰“å¼€æŸ¥çœ‹/ä½¿ç”¨ï¼")
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼š{e}")
