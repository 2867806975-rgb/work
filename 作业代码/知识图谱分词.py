import pandas as pd
import jieba
import json
import re
import os

# ---------------------- ç¬¬ä¸€æ­¥ï¼šè‡ªåŠ¨å®šä½Excelæ–‡ä»¶ï¼ˆæ¡Œé¢/å½“å‰æ–‡ä»¶å¤¹ï¼‰ ----------------------
def find_excel_file(file_name: str) -> str:
    """
    è‡ªåŠ¨æŸ¥æ‰¾Excelæ–‡ä»¶ï¼ˆå…ˆæŸ¥æ¡Œé¢ï¼Œå†æŸ¥å½“å‰æ–‡ä»¶å¤¹ï¼‰
    :param file_name: æ–‡ä»¶åï¼ˆå¦‚"èµµä¸½é¢–è¯„è®ºçˆ¬å–.xlsx"ï¼‰
    :return: å®Œæ•´è·¯å¾„/ç©ºå­—ç¬¦ä¸²ï¼ˆæœªæ‰¾åˆ°ï¼‰
    """
    # æ¡Œé¢è·¯å¾„
    desktop = os.path.join(os.path.expanduser("~"), "Desktop")
    desktop_path = os.path.join(desktop, file_name)
    if os.path.exists(desktop_path):
        print(f"âœ… æ‰¾åˆ°æ–‡ä»¶ï¼š{desktop_path}")
        return desktop_path
    
    # å½“å‰æ–‡ä»¶å¤¹è·¯å¾„
    current_path = os.path.join(os.getcwd(), file_name)
    if os.path.exists(current_path):
        print(f"âœ… æ‰¾åˆ°æ–‡ä»¶ï¼š{current_path}")
        return current_path
    
    # æœªæ‰¾åˆ°ï¼Œæç¤ºç”¨æˆ·
    print(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶ï¼š{file_name}")
    print("ğŸ“Œ è¯·ç¡®è®¤æ–‡ä»¶å­˜åœ¨äºä»¥ä¸‹ä½ç½®ä¹‹ä¸€ï¼š")
    print(f"   1. æ¡Œé¢ï¼š{desktop}")
    print(f"   2. ä»£ç æ‰€åœ¨æ–‡ä»¶å¤¹ï¼š{os.getcwd()}")
    return ""

# è‡ªåŠ¨æŸ¥æ‰¾ä½ çš„Excelæ–‡ä»¶ï¼ˆæ— éœ€æ‰‹åŠ¨æ”¹è·¯å¾„ï¼‰
zly_excel = find_excel_file("èµµä¸½é¢–è¯„è®ºçˆ¬å–.xlsx")
lty_excel = find_excel_file("æ´›å¤©ä¾è¯„è®ºçˆ¬å–.xlsx")

# ---------------------- ç¬¬äºŒæ­¥ï¼šæ•°æ®é¢„å¤„ç†ï¼ˆå®¹é”™ï¼‰ ----------------------
def load_and_clean_comments(excel_path: str, idol_name: str) -> list:
    if not excel_path:
        return []
    try:
        df = pd.read_excel(excel_path)
        # å…¼å®¹ä¸åŒåˆ—åï¼ˆè¯„è®º/è¯„è®ºå†…å®¹/ç²‰ä¸è¯„è®ºï¼‰
        comment_cols = [col for col in df.columns if "è¯„è®º" in col]
        if not comment_cols:
            print(f"âŒ {idol_name}Excelä¸­æ— â€œè¯„è®ºâ€ç›¸å…³åˆ—")
            return []
        comments = df[comment_cols[0]].dropna().astype(str).tolist()
    except Exception as e:
        print(f"âŒ è¯»å–{idol_name}Excelå¤±è´¥ï¼š{str(e)[:50]}")
        return []
    
    # æ¸…æ´—è§„åˆ™
    stop_names = {idol_name, "é¢–å®", "å¤©ä¾", "æ®¿ä¸‹", "èµµå§", "æ´›æ®¿"}
    clean_comments = []
    for c in comments:
        # å‰”é™¤éä¸­æ–‡+å»ç©ºæ ¼
        c_clean = re.sub(r"[^\u4e00-\u9fa5]", "", c).strip()
        # å‰”é™¤å¶åƒå‘½å
        for name in stop_names:
            c_clean = c_clean.replace(name, "")
        # è¿‡æ»¤çŸ­æ–‡æœ¬
        if len(c_clean) >= 3:
            clean_comments.append(c_clean)
    print(f"âœ… {idol_name}æœ‰æ•ˆè¯„è®ºæ•°ï¼š{len(clean_comments)}")
    return clean_comments

# åŠ è½½å¹¶æ¸…æ´—æ•°æ®
zly_comments = load_and_clean_comments(zly_excel, "èµµä¸½é¢–")
lty_comments = load_and_clean_comments(lty_excel, "æ´›å¤©ä¾")
all_comments = zly_comments + lty_comments

if not all_comments:
    print("âŒ æ— æœ‰æ•ˆè¯„è®ºæ•°æ®ï¼Œç¨‹åºé€€å‡º")
    exit()

# ---------------------- ç¬¬ä¸‰æ­¥ï¼šç¦»çº¿å®ä½“/å…³ç³»æŠ½å–ï¼ˆæ— å¤–ç½‘ä¾èµ–ï¼‰ ----------------------
# é¢„è®¾æ ¸å¿ƒå®ä½“åº“ï¼ˆè´´åˆä½ çš„åœºæ™¯ï¼‰
PRESET_ENTITIES = {
    "èµµä¸½é¢–": {"type": "äººç‰©", "sub_type": "çœŸå®å¶åƒ"},
    "æ´›å¤©ä¾": {"type": "äººç‰©", "sub_type": "è™šæ‹Ÿå¶åƒ"},
    "èŠ±åƒéª¨": {"type": "ä½œå“"},
    "é£å¹åŠå¤": {"type": "ä½œå“"},
    "æ¼”å”±ä¼š": {"type": "è¡Œä¸º"},
    "æ­Œæ›²": {"type": "ä½œå“"},
    "æ¼”æŠ€": {"type": "ç‰¹è´¨"},
    "å£°éŸ³": {"type": "ç‰¹è´¨"}
}

# é¢„è®¾æƒ…æ„Ÿè¯åº“
EMOTION_WORDS = {
    "å–œæ¬¢", "çˆ±", "çƒ­çˆ±", "å¼€å¿ƒ", "å¿«ä¹", "æ„ŸåŠ¨", "æš–å¿ƒ", "æƒŠè‰³", "ä¼˜ç§€", "æ£’", "å¥½", "å®Œç¾",
    "èµ", "æ”¯æŒ", "è®¤å¯", "æ¬£èµ", "å¯çˆ±", "å¥½å¬", "è¿‡ç˜¾", "å€¼å¾—", "éª„å‚²", "ç”œèœœ", "æ²»æ„ˆ",
    "å¤±æœ›", "éš¾è¿‡", "ä¸æ»¡", "è®¨åŒ", "å·®", "ä¸å¥½", "é—æ†¾", "åæ§½", "æ— è¯­", "ç”Ÿæ°”"
}

# æŠ½å–å®ä½“å’Œä¸‰å…ƒç»„
entities = []  # æ ¼å¼ï¼š[(å®ä½“å, å®ä½“ç±»å‹), ...]
triples = []   # æ ¼å¼ï¼š[(å¤´å®ä½“, å…³ç³», å°¾å®ä½“), ...]

for comment in all_comments:
    # 1. åŒ¹é…å¶åƒå®ä½“
    for idol, info in PRESET_ENTITIES.items():
        if idol in comment and idol in ["èµµä¸½é¢–", "æ´›å¤©ä¾"]:
            # æ·»åŠ å¶åƒå®ä½“
            entities.append((idol, info["type"]))
            entities.append((info["sub_type"], "å¶åƒç±»å‹"))
            
            # 2. åŒ¹é…æƒ…æ„Ÿè¯ï¼Œç”Ÿæˆæƒ…æ„Ÿå…³ç³»
            for emo in EMOTION_WORDS:
                if emo in comment:
                    triples.append(("ç²‰ä¸", emo, idol))
                    entities.append((emo, "æƒ…æ„Ÿè¯"))
            
            # 3. åŒ¹é…ä½œå“/ç‰¹è´¨ï¼Œç”Ÿæˆå…³è”å…³ç³»
            for entity, e_info in PRESET_ENTITIES.items():
                if entity in comment and entity not in ["èµµä¸½é¢–", "æ´›å¤©ä¾"]:
                    entities.append((entity, e_info["type"]))
                    # ç”Ÿæˆå…³ç³»ï¼ˆæ ¹æ®å®ä½“ç±»å‹ï¼‰
                    if e_info["type"] == "ä½œå“":
                        triples.append((idol, "å…³è”", entity))
                    elif e_info["type"] == "ç‰¹è´¨":
                        triples.append((idol, "æ‹¥æœ‰", entity))

# å»é‡ï¼ˆé¿å…é‡å¤å®ä½“/å…³ç³»ï¼‰
entities = list(set(entities))
triples = list(set([tuple(t) for t in triples]))

# ---------------------- ç¬¬å››æ­¥ï¼šä¿å­˜ç»“æœåˆ°æ¡Œé¢ï¼ˆå¯è§†åŒ–ç”¨ï¼‰ ----------------------
# æ¡Œé¢è·¯å¾„
desktop = os.path.join(os.path.expanduser("~"), "Desktop")

# ä¿å­˜å®ä½“
entity_path = os.path.join(desktop, "çŸ¥è¯†å›¾è°±_å®ä½“.json")
with open(entity_path, "w", encoding="utf-8") as f:
    json.dump(entities, f, ensure_ascii=False, indent=2)

# ä¿å­˜ä¸‰å…ƒç»„
triple_path = os.path.join(desktop, "çŸ¥è¯†å›¾è°±_ä¸‰å…ƒç»„.json")
with open(triple_path, "w", encoding="utf-8") as f:
    json.dump(triples, f, ensure_ascii=False, indent=2)

# ---------------------- ç»“æœæç¤º ----------------------
print("\nğŸ‰ ç¦»çº¿æŠ½å–å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜åˆ°æ¡Œé¢ï¼š")
print(f"1. å®ä½“æ–‡ä»¶ï¼š{entity_path}")
print(f"2. ä¸‰å…ƒç»„æ–‡ä»¶ï¼š{triple_path}")
print("\nğŸ“Œ å®ä½“ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰ï¼š")
for e in entities[:5]:
    print(f"   - {e[0]}ï¼ˆ{e[1]}ï¼‰")
print("\nğŸ“Œ ä¸‰å…ƒç»„ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰ï¼š")
for t in triples[:5]:
    print(f"   - {t[0]} â†’ {t[1]} â†’ {t[1]}")
